/* Copyright Â© 2020 Advanced Micro Devices, Inc. All rights reserved */

#ifndef _CUBEHASH256_H_
#define _CUBEHASH256_H_

#include <hip/hip_runtime.h>
#include "embers/crypto/helpers.h"
#include "embers/helpers/bit_helpers.cuh"

namespace embers
{

namespace crypto
{

namespace cubehash
{

template <int rounds>
__host__ __device__ inline void hash256_rrounds(uint32_t *x)
{
  uint32_t x0[16];
  uint32_t x1[16];

#pragma unroll
  for (int r = 0; r < rounds; r += 2) {
    x0[0x0] = rotate<uint32_t, 7>(x[0x0]);
    x0[0x1] = rotate<uint32_t, 7>(x[0x1]);
    x0[0x2] = rotate<uint32_t, 7>(x[0x2]);
    x0[0x3] = rotate<uint32_t, 7>(x[0x3]);
    x0[0x4] = rotate<uint32_t, 7>(x[0x4]);
    x0[0x5] = rotate<uint32_t, 7>(x[0x5]);
    x0[0x6] = rotate<uint32_t, 7>(x[0x6]);
    x0[0x7] = rotate<uint32_t, 7>(x[0x7]);
    x0[0x8] = rotate<uint32_t, 7>(x[0x8]);
    x0[0x9] = rotate<uint32_t, 7>(x[0x9]);
    x0[0xa] = rotate<uint32_t, 7>(x[0xa]);
    x0[0xb] = rotate<uint32_t, 7>(x[0xb]);
    x0[0xc] = rotate<uint32_t, 7>(x[0xc]);
    x0[0xd] = rotate<uint32_t, 7>(x[0xd]);
    x0[0xe] = rotate<uint32_t, 7>(x[0xe]);
    x0[0xf] = rotate<uint32_t, 7>(x[0xf]);

    x1[0x0] = x[0x10] + x[0x0];
    x1[0x1] = x[0x11] + x[0x1];
    x1[0x2] = x[0x12] + x[0x2];
    x1[0x3] = x[0x13] + x[0x3];
    x1[0x4] = x[0x14] + x[0x4];
    x1[0x5] = x[0x15] + x[0x5];
    x1[0x6] = x[0x16] + x[0x6];
    x1[0x7] = x[0x17] + x[0x7];
    x1[0x8] = x[0x18] + x[0x8];
    x1[0x9] = x[0x19] + x[0x9];
    x1[0xa] = x[0x1a] + x[0xa];
    x1[0xb] = x[0x1b] + x[0xb];
    x1[0xc] = x[0x1c] + x[0xc];
    x1[0xd] = x[0x1d] + x[0xd];
    x1[0xe] = x[0x1e] + x[0xe];
    x1[0xf] = x[0x1f] + x[0xf];

    x[0x0] = x0[0x0] ^ x1[0x8];
    x[0x1] = x0[0x1] ^ x1[0x9];
    x[0x2] = x0[0x2] ^ x1[0xa];
    x[0x3] = x0[0x3] ^ x1[0xb];
    x[0x4] = x0[0x4] ^ x1[0xc];
    x[0x5] = x0[0x5] ^ x1[0xd];
    x[0x6] = x0[0x6] ^ x1[0xe];
    x[0x7] = x0[0x7] ^ x1[0xf];
    x[0x8] = x0[0x8] ^ x1[0x0];
    x[0x9] = x0[0x9] ^ x1[0x1];
    x[0xa] = x0[0xa] ^ x1[0x2];
    x[0xb] = x0[0xb] ^ x1[0x3];
    x[0xc] = x0[0xc] ^ x1[0x4];
    x[0xd] = x0[0xd] ^ x1[0x5];
    x[0xe] = x0[0xe] ^ x1[0x6];
    x[0xf] = x0[0xf] ^ x1[0x7];

    x0[0x0] = rotate<uint32_t, 11>(x[0x0]);
    x0[0x1] = rotate<uint32_t, 11>(x[0x1]);
    x0[0x2] = rotate<uint32_t, 11>(x[0x2]);
    x0[0x3] = rotate<uint32_t, 11>(x[0x3]);
    x0[0x4] = rotate<uint32_t, 11>(x[0x4]);
    x0[0x5] = rotate<uint32_t, 11>(x[0x5]);
    x0[0x6] = rotate<uint32_t, 11>(x[0x6]);
    x0[0x7] = rotate<uint32_t, 11>(x[0x7]);
    x0[0x8] = rotate<uint32_t, 11>(x[0x8]);
    x0[0x9] = rotate<uint32_t, 11>(x[0x9]);
    x0[0xa] = rotate<uint32_t, 11>(x[0xa]);
    x0[0xb] = rotate<uint32_t, 11>(x[0xb]);
    x0[0xc] = rotate<uint32_t, 11>(x[0xc]);
    x0[0xd] = rotate<uint32_t, 11>(x[0xd]);
    x0[0xe] = rotate<uint32_t, 11>(x[0xe]);
    x0[0xf] = rotate<uint32_t, 11>(x[0xf]);

    x[0x1a] = x1[0xa] + x[0x0];
    x[0x1b] = x1[0xb] + x[0x1];
    x[0x18] = x1[0x8] + x[0x2];
    x[0x19] = x1[0x9] + x[0x3];
    x[0x1e] = x1[0xe] + x[0x4];
    x[0x1f] = x1[0xf] + x[0x5];
    x[0x1c] = x1[0xc] + x[0x6];
    x[0x1d] = x1[0xd] + x[0x7];
    x[0x12] = x1[0x2] + x[0x8];
    x[0x13] = x1[0x3] + x[0x9];
    x[0x10] = x1[0x0] + x[0xa];
    x[0x11] = x1[0x1] + x[0xb];
    x[0x16] = x1[0x6] + x[0xc];
    x[0x17] = x1[0x7] + x[0xd];
    x[0x14] = x1[0x4] + x[0xe];
    x[0x15] = x1[0x5] + x[0xf];

    x[0x00] = x0[0x0] ^ x[0x1e];
    x[0x01] = x0[0x1] ^ x[0x1f];
    x[0x02] = x0[0x2] ^ x[0x1c];
    x[0x03] = x0[0x3] ^ x[0x1d];
    x[0x04] = x0[0x4] ^ x[0x1a];
    x[0x05] = x0[0x5] ^ x[0x1b];
    x[0x06] = x0[0x6] ^ x[0x18];
    x[0x07] = x0[0x7] ^ x[0x19];
    x[0x08] = x0[0x8] ^ x[0x16];
    x[0x09] = x0[0x9] ^ x[0x17];
    x[0x0a] = x0[0xa] ^ x[0x14];
    x[0x0b] = x0[0xb] ^ x[0x15];
    x[0x0c] = x0[0xc] ^ x[0x12];
    x[0x0d] = x0[0xd] ^ x[0x13];
    x[0x0e] = x0[0xe] ^ x[0x10];
    x[0x0f] = x0[0xf] ^ x[0x11];

    x0[0x0] = rotate<uint32_t, 7>(x[0x00]);
    x0[0x1] = rotate<uint32_t, 7>(x[0x01]);
    x0[0x2] = rotate<uint32_t, 7>(x[0x02]);
    x0[0x3] = rotate<uint32_t, 7>(x[0x03]);
    x0[0x4] = rotate<uint32_t, 7>(x[0x04]);
    x0[0x5] = rotate<uint32_t, 7>(x[0x05]);
    x0[0x6] = rotate<uint32_t, 7>(x[0x06]);
    x0[0x7] = rotate<uint32_t, 7>(x[0x07]);
    x0[0x8] = rotate<uint32_t, 7>(x[0x08]);
    x0[0x9] = rotate<uint32_t, 7>(x[0x09]);
    x0[0xa] = rotate<uint32_t, 7>(x[0x0a]);
    x0[0xb] = rotate<uint32_t, 7>(x[0x0b]);
    x0[0xc] = rotate<uint32_t, 7>(x[0x0c]);
    x0[0xd] = rotate<uint32_t, 7>(x[0x0d]);
    x0[0xe] = rotate<uint32_t, 7>(x[0x0e]);
    x0[0xf] = rotate<uint32_t, 7>(x[0x0f]);

    x1[0xf] = x[0x1f] + x[0x00];
    x1[0xe] = x[0x1e] + x[0x01];
    x1[0xd] = x[0x1d] + x[0x02];
    x1[0xc] = x[0x1c] + x[0x03];
    x1[0xb] = x[0x1b] + x[0x04];
    x1[0xa] = x[0x1a] + x[0x05];
    x1[0x9] = x[0x19] + x[0x06];
    x1[0x8] = x[0x18] + x[0x07];
    x1[0x7] = x[0x17] + x[0x08];
    x1[0x6] = x[0x16] + x[0x09];
    x1[0x5] = x[0x15] + x[0x0a];
    x1[0x4] = x[0x14] + x[0x0b];
    x1[0x3] = x[0x13] + x[0x0c];
    x1[0x2] = x[0x12] + x[0x0d];
    x1[0x1] = x[0x11] + x[0x0e];
    x1[0x0] = x[0x10] + x[0x0f];

    x[0x00] = x0[0x0] ^ x1[0x7];
    x[0x01] = x0[0x1] ^ x1[0x6];
    x[0x02] = x0[0x2] ^ x1[0x5];
    x[0x03] = x0[0x3] ^ x1[0x4];
    x[0x04] = x0[0x4] ^ x1[0x3];
    x[0x05] = x0[0x5] ^ x1[0x2];
    x[0x06] = x0[0x6] ^ x1[0x1];
    x[0x07] = x0[0x7] ^ x1[0x0];
    x[0x08] = x0[0x8] ^ x1[0xf];
    x[0x09] = x0[0x9] ^ x1[0xe];
    x[0x0a] = x0[0xa] ^ x1[0xd];
    x[0x0b] = x0[0xb] ^ x1[0xc];
    x[0x0c] = x0[0xc] ^ x1[0xb];
    x[0x0d] = x0[0xd] ^ x1[0xa];
    x[0x0e] = x0[0xe] ^ x1[0x9];
    x[0x0f] = x0[0xf] ^ x1[0x8];

    x0[0x0] = rotate<uint32_t, 11>(x[0x00]);
    x0[0x1] = rotate<uint32_t, 11>(x[0x01]);
    x0[0x2] = rotate<uint32_t, 11>(x[0x02]);
    x0[0x3] = rotate<uint32_t, 11>(x[0x03]);
    x0[0x4] = rotate<uint32_t, 11>(x[0x04]);
    x0[0x5] = rotate<uint32_t, 11>(x[0x05]);
    x0[0x6] = rotate<uint32_t, 11>(x[0x06]);
    x0[0x7] = rotate<uint32_t, 11>(x[0x07]);
    x0[0x8] = rotate<uint32_t, 11>(x[0x08]);
    x0[0x9] = rotate<uint32_t, 11>(x[0x09]);
    x0[0xa] = rotate<uint32_t, 11>(x[0x0a]);
    x0[0xb] = rotate<uint32_t, 11>(x[0x0b]);
    x0[0xc] = rotate<uint32_t, 11>(x[0x0c]);
    x0[0xd] = rotate<uint32_t, 11>(x[0x0d]);
    x0[0xe] = rotate<uint32_t, 11>(x[0x0e]);
    x0[0xf] = rotate<uint32_t, 11>(x[0x0f]);

    x[0x15] = x1[0x5] + x[0x00];
    x[0x14] = x1[0x4] + x[0x01];
    x[0x17] = x1[0x7] + x[0x02];
    x[0x16] = x1[0x6] + x[0x03];
    x[0x11] = x1[0x1] + x[0x04];
    x[0x10] = x1[0x0] + x[0x05];
    x[0x13] = x1[0x3] + x[0x06];
    x[0x12] = x1[0x2] + x[0x07];
    x[0x1d] = x1[0xd] + x[0x08];
    x[0x1c] = x1[0xc] + x[0x09];
    x[0x1f] = x1[0xf] + x[0x0a];
    x[0x1e] = x1[0xe] + x[0x0b];
    x[0x19] = x1[0x9] + x[0x0c];
    x[0x18] = x1[0x8] + x[0x0d];
    x[0x1b] = x1[0xb] + x[0x0e];
    x[0x1a] = x1[0xa] + x[0x0f];

    x[0x00] = x0[0x0] ^ x[0x11];
    x[0x01] = x0[0x1] ^ x[0x10];
    x[0x02] = x0[0x2] ^ x[0x13];
    x[0x03] = x0[0x3] ^ x[0x12];
    x[0x04] = x0[0x4] ^ x[0x15];
    x[0x05] = x0[0x5] ^ x[0x14];
    x[0x06] = x0[0x6] ^ x[0x17];
    x[0x07] = x0[0x7] ^ x[0x16];
    x[0x08] = x0[0x8] ^ x[0x19];
    x[0x09] = x0[0x9] ^ x[0x18];
    x[0x0a] = x0[0xa] ^ x[0x1b];
    x[0x0b] = x0[0xb] ^ x[0x1a];
    x[0x0c] = x0[0xc] ^ x[0x1d];
    x[0x0d] = x0[0xd] ^ x[0x1c];
    x[0x0e] = x0[0xe] ^ x[0x1f];
    x[0x0f] = x0[0xf] ^ x[0x1e];
  }
}

template <int rounds = 16>
__host__ __device__ inline void hash256_single(hash256_t *my_hash)
{
  // initialization vector
  uint32_t x[32] = {0xEA2BD4B4, 0xCCD6F29F, 0x63117E71, 0x35481EAE, 0x22512D5B, 0xE5D94E63,
                    0x7E624131, 0xF4CC12BE, 0xC2D0B696, 0x42AF2070, 0xD0720C35, 0x3361DA8C,
                    0x28CCECA4, 0x8EF8AD83, 0x4680AC00, 0x40E5FBAB, 0xD89041C3, 0x6107FBD5,
                    0x6C859D41, 0xF0B26679, 0x09392549, 0x5FA25603, 0x65C892FD, 0x93CB6285,
                    0x2AF2B5AE, 0x9E4B4E60, 0x774ABFDD, 0x85254725, 0x15815AEB, 0x4AB6AAD6,
                    0x9CDAF8AF, 0xD6032C0A};

// XOR IV into x[0-7]
#pragma unroll
  for (auto i = 0; i < HASH256_H4_NUM_INDEXES; i++) {
    x[i] ^= my_hash->h4[i];
  }

  hash256_rrounds<rounds>(x);
  x[0] ^= 0x80U;

  hash256_rrounds<rounds>(x);
  x[0x1f] ^= 1U;

#pragma unroll
  for (auto i = 0; i < 10; ++i) {
    hash256_rrounds<rounds>(x);
  }

#pragma unroll
  for (auto i = 0; i < HASH256_H4_NUM_INDEXES; i++) {
    my_hash->h4[i] = x[i];
  }
}

template <int rounds>
__device__ inline void hash256(hash256_t *hashes)
{
  auto tid = threadIdx.x + blockDim.x * blockIdx.x;
  auto my_hash = &hashes[tid];
  hash256_single<rounds>(my_hash);
}

template <int rounds = 16>
__launch_bounds__(64) __global__ void g_hash256_rounds(hash256_t *hashes)
{
  hash256<rounds>(hashes);
}

__launch_bounds__(64) __global__ void g_hash256(hash256_t *hashes)
{
  static constexpr int hash256_default_num_rounds = 16;
  hash256<hash256_default_num_rounds>(hashes);
}

}  // namespace cubehash
}  // namespace crypto
}  // namespace embers
#endif  // _CUBEHASH256_H_
